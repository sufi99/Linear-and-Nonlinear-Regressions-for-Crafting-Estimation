{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Linear and Nonlinear Regressions for Crafting Estimation",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z1V7C7KstdU"
      },
      "source": [
        "**Crafting Time Estimation**\n",
        "\n",
        "We will be using regressions, of both the linear and non-linear persuasions.  We will be using these to create estimates in the amount of time required to get from a given crafting state to another crafting state, i.e. to produce a heuristic cost for the planning done in homework 2.  \n",
        "\n",
        "\n",
        "We will be constructing the following models:\n",
        "\n",
        "1. A linear regression solved via raw matrix operations, as discussed in class\n",
        "2. A linear regression solved via numpy's library\n",
        "3. A linear regression solved via Stochastic Gradient Descent with an artificial neural network\n",
        "4. A linear regression using a deep artificial neural network\n",
        "5. A non-linear regression using a deep artificial neural network\n",
        "\n",
        "Finally, the non-linear regression will be used as the heuristic in an A* search of the planning space "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb1Acg0nstdV"
      },
      "source": [
        "First step is to read the data in a form that is conducive for regression.  The data is a CSV file where the first row are the names of each column.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNUzFiOpsvM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4fce4f-6247-4588-e3a1-10340715f6cb"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/adamsumm/AI_Minecraft_Assignments/master/CraftingRegressionEstimation/crafting_times.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-05 23:18:15--  https://raw.githubusercontent.com/adamsumm/AI_Minecraft_Assignments/master/CraftingRegressionEstimation/crafting_times.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2128878 (2.0M) [text/plain]\n",
            "Saving to: ‘crafting_times.csv’\n",
            "\n",
            "crafting_times.csv  100%[===================>]   2.03M  13.1MB/s    in 0.2s    \n",
            "\n",
            "2020-11-05 23:18:16 (13.1 MB/s) - ‘crafting_times.csv’ saved [2128878/2128878]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uXNwf_XstdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab206c0-72bf-4895-e6f7-e6c625aa1000"
      },
      "source": [
        "import numpy as np\n",
        "#Open the file\n",
        "with open('crafting_times.csv','r') as infile:\n",
        "    #Get the header line\n",
        "    header = infile.readline().rstrip().split(',')\n",
        "    data = []\n",
        "    #Read it in\n",
        "    for line in infile:\n",
        "        data.append([float(s) for s in line.rstrip().split(',')])\n",
        "    #turn our list of lists into a numpy array\n",
        "    data = np.array(data)\n",
        "    \n",
        "print('\\n'.join(header))\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time\n",
            "initial_bench\n",
            "initial_cart\n",
            "initial_coal\n",
            "initial_cobble\n",
            "initial_furnace\n",
            "initial_ingot\n",
            "initial_iron_axe\n",
            "initial_iron_pickaxe\n",
            "initial_ore\n",
            "initial_plank\n",
            "initial_rail\n",
            "initial_stick\n",
            "initial_stone_axe\n",
            "initial_stone_pickaxe\n",
            "initial_wood\n",
            "initial_wooden_axe\n",
            "initial_wooden_pickaxe\n",
            "goal_bench\n",
            "goal_cart\n",
            "goal_coal\n",
            "goal_cobble\n",
            "goal_furnace\n",
            "goal_ingot\n",
            "goal_iron_axe\n",
            "goal_iron_pickaxe\n",
            "goal_ore\n",
            "goal_plank\n",
            "goal_rail\n",
            "goal_stick\n",
            "goal_stone_axe\n",
            "goal_stone_pickaxe\n",
            "goal_wood\n",
            "goal_wooden_axe\n",
            "goal_wooden_pickaxe\n",
            "(30000, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWJ6znOEstdY"
      },
      "source": [
        "We see that the columns are: \n",
        "0 -- The time it takes\n",
        "1-17 -- The initial state\n",
        "18-35 -- The goal state\n",
        "\n",
        "Now, we need to construct our X and Y matrices.\n",
        "\n",
        "Luckily, *slicing* is very easy to do with numpy arrays.  *Slicing* is where we can easily specify how to take subsets of our matrix.  Think of it like indexing into an array, only we can do a lot of them at once.\n",
        "\n",
        "\n",
        "The general syntax is:\n",
        "\n",
        "`vector[a:b]`\n",
        "`matrix[a:b,c:d]` \n",
        "`tensor[a:b,c:d,e:f]`\n",
        "\n",
        "As a note, `a, b, c, d, e, f` are the indices you wish to get -- if `a` is blank it will start from the beginning and if `b` is blank it will go until the end.  Note: these can also be negative, which can be thought of as `n` away from the end.\n",
        "\n",
        "Some examples:\n",
        "\n",
        "`data[:,0]` -- Get all of the members of the first column \n",
        "`data[:,-1]` -- Get all of the members of the last column\n",
        "`data[:a,1:]` -- Get the first `a` rows for the 2nd to last columns\n",
        "`data[a:,1:]` -- Get all of the rows starting at `a` for the 2nd to last columns\n",
        "\n",
        "As a note, you can get the dimensions of a numpy array by accessing `.shape`, a tuple of the dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDbA5TujtMPi"
      },
      "source": [
        "#Step 1 -- Set up Matrics  (15 pts)\n",
        "* Let the first N*validation_split rows be for the validation set \n",
        "* and the last N*(1-validation_split) rows be the training data\n",
        "* At the end of this, you should have:\n",
        "    \n",
        "    `Y.shape = (21000, 1)`\n",
        "\n",
        "    `X.shape = (21000, 34)`\n",
        "\n",
        "    `Y_validation.shape = (9000, 1)`\n",
        "\n",
        "    `X_validation.shape = (9000, 34)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ27xt85stdY"
      },
      "source": [
        "#We want to use a training/validation split to verify we are doing a good job\n",
        "validation_split = 0.3\n",
        "\n",
        "\n",
        "#TODO slice the data into the correct matrices for training and validation splits \n",
        "#Let the first N*validation_split rows be for the validation set \n",
        "#and the last N*(1-validation_split) rows be the training data\n",
        "\n",
        "Y = None \n",
        "Y_validation = None \n",
        "\n",
        "X = None \n",
        "X_validation = None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Vgb6pUstda"
      },
      "source": [
        "Now we will use Least Squares Regression to estimate the time cost associated with a given state and end state.  \n",
        "\n",
        "The least squares regression coefficients can be calculated via the closed form solution:\n",
        "\n",
        "$\\beta =  (X^T X)^{-1} X^T Y$\n",
        "\n",
        "First try it out with using `np.dot` (anywhere there is a matrix multiplication) and `np.inv` (anywhere there is a matrix inversion. (as a note, matrix transposition is accomplished with `.T`)\n",
        "\n",
        "Next, compare using `np.linalg.lstsq` -- numpy's built in least squares regression (that is much more stable than using the matrix inversion found here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0hgr3gquF-0"
      },
      "source": [
        "#Step 2 -- Perform Linear Regressions (10 pts)\n",
        "* Write your own version of linear regression using the closed from solution discussed above\n",
        "* Use the supplied least squares regression that is part of numpy's linear algebra library\n",
        "* Compare the two\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7GbcxsGstdb"
      },
      "source": [
        "#Least Squares Estimation Goes Here\n",
        "\n",
        "\n",
        "#TODO replace the np.zeros() with the correct code\n",
        "\n",
        "def calculate_weights_with_linear_algebra(X: np.array, Y: np.array) -> np.array:\n",
        "    return None\n",
        "\n",
        "def calculate_weights_with_library(X: np.array, Y: np.array) -> np.array:\n",
        "    return None\n",
        "\n",
        "B_raw = calculate_weights_with_linear_algebra(X,Y)\n",
        "B_lstsq = calculate_weights_with_library(X,Y)\n",
        "\n",
        "#This should be small, mostly in the 1e-13 to 1e-14 range\n",
        "print(B_raw-B_lstsq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oijgukrZstdd"
      },
      "source": [
        "Now we want to test our coefficients and see how well we predict the answer.  To do with we will need to use the weight vector we just learned.  Use `np.dot` to calculate:\n",
        "\n",
        "$\\hat{Y} = X\\beta$\n",
        "\n",
        "We will then calculate the *residual* -- the error that remains between our true times in Y and the calculated times in Yhat.\n",
        "\n",
        "$resid = Y-\\hat{Y} $\n",
        "\n",
        "We will then use these residuals to come up with a single number that tells us how well we did.  For this, we will be using the Root Mean Squared Error (RMSE)\n",
        "\n",
        "$RMSE = \\sqrt{\\frac{1}{N} \\sum (y-\\hat{y})^2}$\n",
        "\n",
        "To do this we will use the elementwise multiplication (`a*b` not `np.dot(a,b)`), the square root (`np.sqrt`), and mean (``np.mean``) functions\n",
        "\n",
        "#Step 3 -- Inference (5 pts)         \n",
        "* Calculate the predicted values \n",
        "* Calculate the error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8gYatM5stdd"
      },
      "source": [
        "#TODO: Calculate Yhat, the residuals and RMSE for both the training and validation sets\n",
        "\n",
        "def calculate_yhat(X: np.array, B: np.array) -> np.array:\n",
        "    return None\n",
        "\n",
        "def calculate_residuals(Y: np.array, Yhat: np.array) -> np.array:\n",
        "    return None\n",
        "\n",
        "def calculate_rmse(residuals: np.array) -> float:\n",
        "    return 0\n",
        "\n",
        "\n",
        "Yhat = calculate_yhat(X, B_raw)\n",
        "Yhat_validation = calculate_yhat(X_validation, B_raw)\n",
        "\n",
        "residuals  = calculate_residuals(Y, Yhat)\n",
        "residuals_validation  = calculate_residuals(Y_validation, Yhat_validation)\n",
        "\n",
        "rmse = calculate_rmse(residuals)\n",
        "rmse_validation = calculate_rmse(residuals_validation)\n",
        "\n",
        "print('RMSE:',rmse)\n",
        "print('RMSE Validation:',rmse_validation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsJTSFI-stdf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Now let's plot our points residuals\n",
        "#Often, we'd like to plot our data, but we have a 30+dimensional space, i.e. one that's hard to visualize\n",
        "plt.plot(Y,residuals,'x')\n",
        "plt.plot(Y_validation,residuals_validation,'r.')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO2bk4lJstdi"
      },
      "source": [
        "Previously, we learned a weight vector, but because we didn't have a bias term, the weight vector has to go through the origin, which might not be what we want.  Let's try it all again with a bias term this time.\n",
        "\n",
        "To add a bias term, we will add a new column to our X matrix that is full of constants.  \n",
        "\n",
        "Does it matter what constant term we choose?\n",
        "\n",
        "The simplest way to do this is to use `hstack` which takes in a list of matrices and horizontally concatenates them (i.e. adds on new columns -- there exists a `vstack` that adds new rows).  The simpleest way to construct a constant term is to use `np.ones` which takes in a list with the number of ones to make for each dimension.\n",
        "\n",
        "e.g.\n",
        "`np.ones([4,2])` will make\n",
        "\n",
        "1 1\n",
        "\n",
        "1 1 \n",
        "\n",
        "1 1\n",
        "\n",
        "1 1\n",
        "\n",
        "#Step 4 -- Add a bias term (5 pts)\n",
        "* Add a bias term to the independent data -- `X`\n",
        "* Rerun the previous code \n",
        "* Compare the new errors to the old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soScpJOBstdi"
      },
      "source": [
        "#TODO construct an X matrix with a bias term. \n",
        "\n",
        "X_with_bias = np.zeros(10)\n",
        "X_validation_with_bias = np.zeros(10)\n",
        "\n",
        "#TODO replace the np.zeros() with the correct code\n",
        "B_with_bias = calculate_weights_with_library(X_with_bias,Y)\n",
        "\n",
        "Yhat_with_bias = calculate_yhat(X_with_bias, B_with_bias)\n",
        "Yhat_validation_with_bias = calculate_yhat(X_validation_with_bias, B_with_bias)\n",
        "\n",
        "residuals_with_bias  = calculate_residuals(Y, Yhat_with_bias)\n",
        "residuals_validation_with_bias  = calculate_residuals(Y_validation, Yhat_validation_with_bias)\n",
        "\n",
        "rmse_with_bias = calculate_rmse(residuals_with_bias)\n",
        "rmse_validation_with_bias = calculate_rmse(residuals_validation_with_bias)\n",
        "\n",
        "\n",
        "print('RMSE with bias term:',rmse_with_bias)\n",
        "print('RMSE Validation with bias term:',rmse_validation_with_bias)\n",
        "\n",
        "plt.plot(Y,residuals_with_bias,'x')\n",
        "plt.plot(Y_validation,residuals_validation_with_bias,'ro')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eUYTFNNstdk"
      },
      "source": [
        "Now we are going to use artificial neural networks. We are going to be using PyTorch, one of the leading deep learning libraries. \n",
        "\n",
        "NOTE: We are going to be doing this in a GPU enabled way, so be sure to make sure your runtime is set to use a GPU -- Runtime > Change Runtime Type > Hardware Accelerator = GPU\n",
        "\n",
        "\n",
        "First lets use stochastic gradient descent to train a weight vector as we did above.  \n",
        "\n",
        "PyTorch lets us do this in a number of ways, but we will be doing the easiest possible one.  We are going to construct a `Sequential` model, with a `Linear` layer as its sole argument. `Sequential` can take in an arbitrary number of arguments, where each one is a layer that will be applied in the order that it is passed in.\n",
        "\n",
        "The parameters you care about for `Linear` are:\n",
        "\n",
        "`Linear(in_features, out_features, bias=True)`\n",
        "    \n",
        "`in_features` is the dimensionality of our input space -- in this case it will be the number of columns found in our X data\n",
        "`out_features` is the dimensionality of the output space -- in this case, it will be 1 (all of our final `out_features` will always be 1, as our output is the single number we are predicting).  \n",
        "\n",
        "# Step 5 -- Artificial Neural Network (15 pts)\n",
        "* Construct a linear regression model in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCcqAXBWstdl"
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "#TODO construct the model\n",
        "# Define a neural network model as a stack of layers\n",
        "model = torch.nn.Sequential(\n",
        "    \n",
        ")\n",
        "model.to('cuda')\n",
        "\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXkzI4KNstdn"
      },
      "source": [
        "Given our model, it's now time to train it.  First we need to convert our numpy matrices into PyTorch Tensors.  \n",
        "\n",
        "Then we need to set up a couple of things --\n",
        "\n",
        "First, we need to choose which optimizer we are going to use.  For this, let's just go with simple stochastic gradient descent.  `torch.optim.SGD(model.parameters(),lr=LR)` -- you'll need to pick a learning rate.  It's usually best to pick something relatively small, like say 0.01.\n",
        "\n",
        "Then we need to choose our loss function.  If our goal is to do a regression, we should choose the loss function we chose before, i.e. Mean Square Error --  `torch.nn.MSELoss()`\n",
        "\n",
        "\n",
        "Then, we need to loop over our dataset a number of times, i.e. a number of *epochs*.  At each step of the process we need to:\n",
        "\n",
        "1. Zero the gradients from the previous epoch -- `optimizer.zero_grad()` and `model.zero_grad()`\n",
        "2. Run the model in the forward direction -- `Yhat = model.forward(Xt)`\n",
        "3. Calculate the loss between our predictions and the truth -- `loss = loss_fn(Yhat,Yt)`\n",
        "4. Calculate back propagation of the loss through the network -- `loss.backward()`\n",
        "5. Run the stochastic gradient descent and update the weights -- `optimizer.step()`\n",
        "\n",
        "Some libraries hide all of these aspects, but PyTorch makes you do them explicitly.  It results in a little more code, but allows for some very fancy models (those involving different losses being calculated independently) to be done with very little change in the code.\n",
        "\n",
        "#Step 6 -- Training Your Model (20 pts)\n",
        "* Set up your training method\n",
        "* Set up your loss function \n",
        "* Run the training process as defined above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Xd0VrZX7stdn"
      },
      "source": [
        "\n",
        "# Convert our numpy arrays to torch tensors\n",
        "Xt = torch.Tensor(X).to('cuda')\n",
        "# To make Yt match the shape of Yhat, we'll need it to be a slightly different shape\n",
        "Yt = torch.Tensor(Y.reshape((len(Y), 1))).to('cuda')\n",
        "\n",
        "\n",
        "\n",
        "def train(X: torch.Tensor, Y: torch.Tensor, model: torch.nn.Module, epochs:int) -> None:\n",
        "    #TODO set the optimizer and loss functions\n",
        "    optimizer = None\n",
        "\n",
        "    #TODO set the loss function\n",
        "    # We'll use mean squared error as our loss function\n",
        "    loss_fn = None\n",
        "    for t in range(epochs):\n",
        "\n",
        "        #TODO do the training steps here\n",
        "        #1. zero the gradient buffers\n",
        "        #1. Clear out the \"gradient\", i.e. the old update amounts\n",
        "\n",
        "        #2. Make a prediction\n",
        "\n",
        "        #3. Calculate loss (the error of the residual)\n",
        "\n",
        "        if t % 100 == 0:\n",
        "            print(t,loss.item())\n",
        "\n",
        "        #4. Run the loss backwards through the graph\n",
        "\n",
        "        #5. Run the optimizer to update the weights\n",
        "    \n",
        "train(Xt,Yt,model, 5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgzxLrtO9GRQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaRNJb0nstdp"
      },
      "source": [
        "Now we want to see how it did.  We will plot the residuals (i.e. the error) for both our training set and our validation set.  It is always important to have a validation set, as it will let us see how well our model is over (or under) fitting the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OISue2Nmstdp"
      },
      "source": [
        "Yhat = model.forward(Xt).data.numpy()\n",
        "\n",
        "residual  = calculate_residuals(Y, Yhat)\n",
        "\n",
        "plt.plot(Y,residual,'x')\n",
        "\n",
        "\n",
        "Yhat_validation = model.forward(torch.Tensor(X_validation)).data.numpy()\n",
        "\n",
        "residual_validation  = calculate_residuals(Y_validation, Yhat_validation)\n",
        "\n",
        "plt.plot(Y_validation,residual_validation,'ro')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('RMSE:',calculate_rmse(residual))\n",
        "print('Validation RMSE:',calculate_rmse(residual_validation))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKdcYqlstdr"
      },
      "source": [
        "...And we do just about the same as we did before.\n",
        "\n",
        "Now let's try it with some hidden layers.  Instead of just 1 Linear layer, we will have multiple layers.  As with the last one, we will have to specify the in_dimensions (identical to that one).  However, instead of an out_dimension of 1, we will go to the number of hidden_units, let's say 100.  Then we will have an output layer, which will go from our hidden_units dimension to an out_dimension of 1.\n",
        "\n",
        "Again, your model summary should look similar to below (layer weights will be different)\n",
        "\n",
        "#Step 7 -- Multi Layer Neural Network (10 pts)\n",
        "* Create a neural network with 1 hidden layer (i.e. 2 layers, Input -> Hidden, Hidden -> Output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORQMNRn2stds"
      },
      "source": [
        "hidden_units = 100\n",
        "#TODO construct the model\n",
        "# Define a neural network model as a stack of layers\n",
        "model = torch.nn.Sequential(\n",
        "   \n",
        ")\n",
        "\n",
        "model.to('cuda')\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEZom1Glstdu"
      },
      "source": [
        "Copy your training code from above and let's see how well it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yKabh3yAstdv"
      },
      "source": [
        "    \n",
        "train(Xt,Yt,model, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-SI1yGmstdx"
      },
      "source": [
        "Hmmmm.....that's no good. Our loss quickly explodes and goes to nan.  This is cause by our stochastic gradient descent ping-ponging back and forth.  Instead of converging it keeps overshooting more and more until it goes beyond the floating point limit.  Obviously, that isn't what we want.\n",
        "\n",
        "We can address this in one of 2 ways:\n",
        "\n",
        "1. We can shrink out learning rate to a small enough value that this no longer occurs\n",
        "2. We can clip our gradients to make sure they don't exceed a specific value\n",
        "\n",
        "We can do it the first way, but that will slow our training.  The second can be achieved by adding a step into our training process:\n",
        "\n",
        "\n",
        "1. Zero the gradients from the previous epoch -- `optimizer.zero_grad()` and `model.zero_grad()`\n",
        "2. Run the model in the forward direction -- `Yhat = model.forward(Xt)`\n",
        "3. Calculate the loss between our predictions and the truth -- `loss = loss_fn(Yhat,Yt)`\n",
        "4. Calculate back propagation of the loss through the network -- `loss.backward()`\n",
        "5. **Clip the gradients** -- `torch.nn.utils.clip_grad_norm_(model.parameters(),5)`\n",
        "6. Run the stochastic gradient descent and update the weights -- `optimizer.step()`\n",
        "\n",
        "Copy your model construction with the hidden layers from above, and then set up an optimization loop with gradient clipping.\n",
        "\n",
        "#Step 8 -- Gradient Clipping  (10 pts)\n",
        "* Modify your training method from step 6 to add in Gradient Clipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw5nDqOMstdx"
      },
      "source": [
        "\n",
        "def train_with_gradient_clipping(X: torch.Tensor, Y: torch.Tensor, model: torch.nn.Module, epochs:int) -> None:\n",
        "    #TODO set the optimizer and loss functions\n",
        "    optimizer = None\n",
        "\n",
        "    #TODO set the loss function\n",
        "    # We'll use mean squared error as our loss function\n",
        "    loss_fn = None\n",
        "    for t in range(epochs):\n",
        "\n",
        "        #TODO do the training steps here\n",
        "        #1. zero the gradient buffers\n",
        "        #1. Clear out the \"gradient\", i.e. the old update amounts\n",
        "\n",
        "        #2. Make a prediction\n",
        "\n",
        "        #3. Calculate loss (the error of the residual)\n",
        "\n",
        "        if t % 100 == 0:\n",
        "            print(t,loss.item())\n",
        "\n",
        "        #4. Run the loss backwards through the graph\n",
        "\n",
        "        #5. Clip the gradients\n",
        "        \n",
        "        #6. Run the optimizer to update the weights\n",
        "    \n",
        "train_with_gradient_clipping(Xt,Yt,model, 5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFL7IoDUstdz"
      },
      "source": [
        "Yhat = model.forward(Xt).data.numpy()\n",
        "\n",
        "residual  = calculate_residuals(Y, Yhat)\n",
        "\n",
        "plt.plot(Y,residual,'x')\n",
        "\n",
        "\n",
        "Yhat_validation = model.forward(torch.Tensor(X_validation)).data.numpy()\n",
        "\n",
        "residual_validation  = calculate_residuals(Y_validation, Yhat_validation)\n",
        "\n",
        "plt.plot(Y_validation,residual_validation,'ro')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('RMSE:',calculate_rmse(residual))\n",
        "print('Validation RMSE:',calculate_rmse(residual_validation))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT0BjAhBstd1"
      },
      "source": [
        "Wait, that looks just like it did before!  The key to neural networks comes from the non-linear activations.  No matter how many layers we add, so long as the rank of the hidden layers is $\\geq$ the rank of the original vector, the best we can do is the least squares estimation (as it is the maximum likelihood estimator for a linear regression).  If the rank is decreased, then we are doing some form of compression, akin to Principal Component Analysis.  Let's try it with a bit of nonlinearity. \n",
        "\n",
        "Let's do a single hidden layer with a non-linear activation -- we will use the Rectified Linear Unit (ReLU) as it is fast and all we really care about is ANY kind of nonlinearity (sometimes we care about our nonlinearity having a specific meaning or mapping into a specific range (0 to 1, -1 to 1, etc.).\n",
        "\n",
        "This means we should now have a Sequential model with\n",
        "\n",
        "1. A Linear layer going from our input dimension to the number of hidden units\n",
        "2. A ReLU activation layer -- `torch.nn.ReLU()`\n",
        "3. A Linear layer going from our hidden units to 1\n",
        "\n",
        "\n",
        "#Step 9 -- Create a non-linear multi layer Neural Network (10 pts)\n",
        "* The network should be two layers like above, but should have a non-linear activation function (the ReLU) (e.g.  Input -> Hidden, ReLU, Hidden -> Output)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuNkOLVYstd2"
      },
      "source": [
        "hidden_units = 100\n",
        "#TODO construct the model\n",
        "# Define a neural network model as a stack of layers\n",
        "model = torch.nn.Sequential(\n",
        "    \n",
        ")\n",
        "\n",
        "\n",
        "model.to('cuda')\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcdTiQqnstd3"
      },
      "source": [
        "Use the optimization code with the gradient clipping from above to train our new, non-linear, model.  I recommend letting it run for about 10000 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WNF1QC-Fstd4"
      },
      "source": [
        "\n",
        "train_with_gradient_clipping(Xt,Yt,model, 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwFn4E6bstd6"
      },
      "source": [
        "Yhat = model.forward(Xt).data.numpy()\n",
        "\n",
        "residual  = calculate_residuals(Y, Yhat)\n",
        "\n",
        "plt.plot(Y,residual,'x')\n",
        "\n",
        "\n",
        "Yhat_validation = model.forward(torch.Tensor(X_validation)).data.numpy()\n",
        "\n",
        "residual_validation  = calculate_residuals(Y_validation, Yhat_validation)\n",
        "\n",
        "plt.plot(Y_validation,residual_validation,'ro')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('RMSE:',calculate_rmse(residual))\n",
        "print('Validation RMSE:',calculate_rmse(residual_validation))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}